import yt_dlp
import pandas as pd
import json
import os
from typing import List, Dict, Any

# --- C·∫§U H√åNH CHUNG ---
OUTPUT_DIR = 'youtube_multi_keyword_output'
DOWNLOAD_VIDEO_FILES = False # ƒê·∫∑t True n·∫øu b·∫°n mu·ªën t·∫£i c·∫£ file video
NUM_VIDEOS_PER_KEYWORD = 500  # S·ªë l∆∞·ª£ng video mu·ªën l·∫•y th√¥ng tin cho m·ªói t·ª´ kh√≥a

# --- DANH S√ÅCH T·ª™ KH√ìA B·∫†N MU·ªêN CRAWL ---
KEYWORD_LIST = [
    "covid danger"
    
]

# --- C√ÅC H√ÄM H·ªñ TR·ª¢ HI·ªÇN TH·ªä TI·∫æN TR√åNH (Gi·ªØ nguy√™n) ---
class MyLogger:
    def debug(self, msg):
        pass

    def warning(self, msg):
        print(f"C·∫£nh b√°o: {msg}")

    def error(self, msg):
        print(f"L·ªñI: {msg}")

def my_hook(d):
    if d['status'] == 'downloading':
        p = d['_percent_str']
        e = d['_eta_str']
        print(f"Ti·∫øn tr√¨nh: {p} - ETA: {e}", end='\r')
    elif d['status'] == 'finished':
        print(f"Ho√†n th√†nh x·ª≠ l√Ω file {d['filename']}")


# --- H√ÄM CH√çNH: L·∫§Y TH√îNG TIN T·ª™ M·ªòT T·ª™ KH√ìA ---
def crawl_single_keyword(search_term: str, num_videos: int) -> List[Dict[str, Any]]:
    """
    T√¨m ki·∫øm video theo m·ªôt t·ª´ kh√≥a duy nh·∫•t v√† l·∫•y th√¥ng tin.
    """
    search_query = f"ytsearch{num_videos}:{search_term}"
    
    ydl_opts = {
        'simulate': not DOWNLOAD_VIDEO_FILES, 
        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best', 
        'outtmpl': os.path.join(OUTPUT_DIR, '%(title)s.%(ext)s'),
        'noplaylist': True,
        'quiet': True, 
        'logger': MyLogger(),
        'progress_hooks': [my_hook] if DOWNLOAD_VIDEO_FILES else [],
    }

    print(f"\n--- üîé ƒêang t√¨m ki·∫øm {num_videos} video cho t·ª´ kh√≥a: '{search_term}' ---")

    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(search_query, download=False)
            
            if not info or 'entries' not in info or not info['entries']:
                print(f"‚ùå Kh√¥ng t√¨m th·∫•y video n√†o cho t·ª´ kh√≥a: {search_term}")
                return []

            video_data = []
            urls_to_download = []
            for entry in info['entries']:
                if entry:
                    video_info = {
                        'keyword': search_term, # Th√™m c·ªôt t·ª´ kh√≥a ƒë·ªÉ d·ªÖ ph√¢n lo·∫°i sau n√†y
                        'title': entry.get('title'),
                        'url': entry.get('webpage_url'),
                        'duration_sec': entry.get('duration'),
                        'view_count': entry.get('view_count'),
                        'upload_date': entry.get('upload_date'),
                        'channel': entry.get('channel'),
                        'description': entry.get('description', '').replace('\n', ' ')[:100] + '...',
                    }
                    video_data.append(video_info)
                    urls_to_download.append(video_info['url'])

            print(f"‚úÖ ƒê√£ l·∫•y th√†nh c√¥ng th√¥ng tin c·ªßa {len(video_data)} video.")

            # T·∫£i file video n·∫øu ƒë∆∞·ª£c c·∫•u h√¨nh
            if DOWNLOAD_VIDEO_FILES and urls_to_download:
                 print("\n--- üíæ B·∫Øt ƒë·∫ßu t·∫£i xu·ªëng file video (C·∫ßn FFmpeg) ---")
                 # T·∫£i xu·ªëng t·ª´ng video
                 ydl.download(urls_to_download)
            
            return video_data

    except Exception as e:
        print(f"\n‚ùå ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω t·ª´ kh√≥a '{search_term}': {e}")
        return []

# --- H√ÄM L∆ØU D·ªÆ LI·ªÜU T·ªîNG H·ª¢P ---
def save_data(all_data: List[Dict[str, Any]], filename_prefix: str):
    if not all_data:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu t·ªïng h·ª£p ƒë·ªÉ l∆∞u.")
        return

    # 1. L∆∞u v√†o JSON (T·ªïng h·ª£p)
    json_path = os.path.join(OUTPUT_DIR, f'{filename_prefix}_all_data.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=4)
    print(f"\n‚úÖ D·ªØ li·ªáu JSON t·ªïng h·ª£p ƒë√£ l∆∞u t·∫°i: {json_path}")

    # 2. L∆∞u v√†o CSV (T·ªïng h·ª£p)
    csv_path = os.path.join(OUTPUT_DIR, f'{filename_prefix}_all_data.csv')
    df = pd.DataFrame(all_data)
    df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"‚úÖ D·ªØ li·ªáu CSV t·ªïng h·ª£p ƒë√£ l∆∞u t·∫°i: {csv_path}")


# --- CH·∫†Y CH∆Ø∆†NG TR√åNH CH√çNH ---
if __name__ == "__main__":
    
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        
    all_crawled_data = []

    # L·∫∑p qua t·ª´ng t·ª´ kh√≥a trong danh s√°ch
    for keyword in KEYWORD_LIST:
        results = crawl_single_keyword(
            search_term=keyword,
            num_videos=NUM_VIDEOS_PER_KEYWORD
        )
        all_crawled_data.extend(results) # Th√™m k·∫øt qu·∫£ c·ªßa t·ª´ng t·ª´ kh√≥a v√†o danh s√°ch t·ªïng h·ª£p

    # L∆∞u d·ªØ li·ªáu t·ªïng h·ª£p sau khi crawl xong t·∫•t c·∫£ c√°c t·ª´ kh√≥a
    save_data(all_crawled_data, "multi_keyword")

    print("\n=======================================================")
    print(f"HO√ÄN T·∫§T: ƒê√£ x·ª≠ l√Ω {len(KEYWORD_LIST)} t·ª´ kh√≥a v√† t·ªïng h·ª£p {len(all_crawled_data)} m·ª•c d·ªØ li·ªáu.")
    print("=======================================================")